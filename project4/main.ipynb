{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from numpy.fft import fft  # to get amplitudes\n",
    "import pandas as pd\n",
    "import scipy.signal as ss  # for psd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from biosppy.signals import eeg  # signal processing\n",
    "from biosppy.signals import emg  # signal processing\n",
    "from spectrum import arburg\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "PROTOTYPING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64800, 512) (64800, 512) (64800, 512)\n",
      "(43200, 512) (43200, 512) (43200, 512)\n",
      "(64800, 1)\n",
      "(64800, 55) (43200, 55)\n",
      "Time:  30.928338766098022\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# import train sets\n",
    "train_eeg1_raw = pd.read_csv('files/train_eeg1.csv').drop('Id', axis=1).values\n",
    "train_eeg2_raw = pd.read_csv('files/train_eeg2.csv').drop('Id', axis=1).values\n",
    "train_emg_raw = pd.read_csv('files/train_emg.csv').drop('Id', axis=1).values\n",
    "\n",
    "# import test sets\n",
    "test_eeg1_raw = pd.read_csv('files/test_eeg1.csv').drop('Id', axis=1).values\n",
    "test_eeg2_raw = pd.read_csv('files/test_eeg2.csv').drop('Id', axis=1).values\n",
    "test_emg_raw = pd.read_csv('files/test_emg.csv').drop('Id', axis=1).values\n",
    "\n",
    "# import eeg features directly\n",
    "eeg_train = pd.read_csv('files/eeg_feats_train.csv').values\n",
    "eeg_test  = pd.read_csv('files/eeg_feats_test.csv').values\n",
    "\n",
    "# import reduced eeg features by pca (to 45 components - already scaled)\n",
    "eeg_train_red = pd.read_csv('files/eeg_train_pca45.csv').values\n",
    "eeg_test_red  = pd.read_csv('files/eeg_test_pca45.csv').values\n",
    "\n",
    "# import labels\n",
    "train_labels_raw = pd.read_csv('files/train_labels.csv').drop('Id', axis=1).values\n",
    "\n",
    "print(train_eeg1_raw.shape, train_eeg2_raw.shape, train_emg_raw.shape)\n",
    "print(test_eeg1_raw.shape, test_eeg2_raw.shape, test_emg_raw.shape)\n",
    "print(train_labels_raw.shape)\n",
    "print(eeg_train.shape, eeg_test.shape)\n",
    "\n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction for EEG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 64800\n",
      "1000 / 64800\n",
      "2000 / 64800\n",
      "3000 / 64800\n",
      "4000 / 64800\n",
      "5000 / 64800\n",
      "6000 / 64800\n",
      "7000 / 64800\n",
      "8000 / 64800\n",
      "9000 / 64800\n",
      "10000 / 64800\n",
      "11000 / 64800\n",
      "12000 / 64800\n",
      "13000 / 64800\n",
      "14000 / 64800\n",
      "15000 / 64800\n",
      "16000 / 64800\n",
      "17000 / 64800\n",
      "18000 / 64800\n",
      "19000 / 64800\n",
      "20000 / 64800\n",
      "21000 / 64800\n",
      "22000 / 64800\n",
      "23000 / 64800\n",
      "24000 / 64800\n",
      "25000 / 64800\n",
      "26000 / 64800\n",
      "27000 / 64800\n",
      "28000 / 64800\n",
      "29000 / 64800\n",
      "30000 / 64800\n",
      "31000 / 64800\n",
      "32000 / 64800\n",
      "33000 / 64800\n",
      "34000 / 64800\n",
      "35000 / 64800\n",
      "36000 / 64800\n",
      "37000 / 64800\n",
      "38000 / 64800\n",
      "39000 / 64800\n",
      "40000 / 64800\n",
      "41000 / 64800\n",
      "42000 / 64800\n",
      "43000 / 64800\n",
      "44000 / 64800\n",
      "45000 / 64800\n",
      "46000 / 64800\n",
      "47000 / 64800\n",
      "48000 / 64800\n",
      "49000 / 64800\n",
      "50000 / 64800\n",
      "51000 / 64800\n",
      "52000 / 64800\n",
      "53000 / 64800\n",
      "54000 / 64800\n",
      "55000 / 64800\n",
      "56000 / 64800\n",
      "57000 / 64800\n",
      "58000 / 64800\n",
      "59000 / 64800\n",
      "60000 / 64800\n",
      "61000 / 64800\n",
      "62000 / 64800\n",
      "63000 / 64800\n",
      "64000 / 64800\n",
      "0 / 43200\n",
      "1000 / 43200\n",
      "2000 / 43200\n",
      "3000 / 43200\n",
      "4000 / 43200\n",
      "5000 / 43200\n",
      "6000 / 43200\n",
      "7000 / 43200\n",
      "8000 / 43200\n",
      "9000 / 43200\n",
      "10000 / 43200\n",
      "11000 / 43200\n",
      "12000 / 43200\n",
      "13000 / 43200\n",
      "14000 / 43200\n",
      "15000 / 43200\n",
      "16000 / 43200\n",
      "17000 / 43200\n",
      "18000 / 43200\n",
      "19000 / 43200\n",
      "20000 / 43200\n",
      "21000 / 43200\n",
      "22000 / 43200\n",
      "23000 / 43200\n",
      "24000 / 43200\n",
      "25000 / 43200\n",
      "26000 / 43200\n",
      "27000 / 43200\n",
      "28000 / 43200\n",
      "29000 / 43200\n",
      "30000 / 43200\n",
      "31000 / 43200\n",
      "32000 / 43200\n",
      "33000 / 43200\n",
      "34000 / 43200\n",
      "35000 / 43200\n",
      "36000 / 43200\n",
      "37000 / 43200\n",
      "38000 / 43200\n",
      "39000 / 43200\n",
      "40000 / 43200\n",
      "41000 / 43200\n",
      "42000 / 43200\n",
      "43000 / 43200\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2aadf25068c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_eeg1_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_eeg2_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_emg_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# save features for future imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'files/eeg_feats_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tests\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'files/eeg_feats_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3200\u001b[0m         \"\"\"\n\u001b[0;32m   3201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3202\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3204\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsvs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCSVFormatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_frame'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    " \n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    " \n",
    "def get_features(list_values):\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return crossings + statistics\n",
    "\n",
    "def extract_features(eeg1, eeg2, emg):\n",
    "    features = None\n",
    "    \n",
    "    for i in range(eeg1.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(i, \"/\", eeg1.shape[0])\n",
    "        row = np.array([])\n",
    "\n",
    "        signal = np.array([eeg1[i], eeg2[i]]).T\n",
    "        analysis = eeg.eeg(signal=signal, sampling_rate=128, show=False)    \n",
    "\n",
    "        # theta\n",
    "        row = np.append(row, get_features(analysis[\"theta\"]))\n",
    "        # row = np.append(row, get_features(analysis[\"theta\"][:, 1]))\n",
    "\n",
    "        # alpha low\n",
    "        row = np.append(row, get_features(analysis[\"alpha_low\"]))\n",
    "        # row = np.append(row, get_features(analysis[\"alpha_low\"][:, 1]))\n",
    "\n",
    "        # alpha low\n",
    "        row = np.append(row, get_features(analysis[\"alpha_high\"]))\n",
    "        # row = np.append(row, get_features(analysis[\"alpha_high\"][:, 1]))\n",
    "\n",
    "        # beta\n",
    "        row = np.append(row, get_features(analysis[\"beta\"]))\n",
    "        # row = np.append(row, get_features(analysis[\"beta\"][:, 1]))\n",
    "\n",
    "        # gamma\n",
    "        row = np.append(row, get_features(analysis[\"gamma\"][:, 0]))\n",
    "        # row = np.append(row, get_features(analysis[\"gamma\"]))\n",
    "\n",
    "        # format\n",
    "        row = row.reshape((1, -1))\n",
    "\n",
    "        # concatenate\n",
    "        if features is None:\n",
    "            features = row\n",
    "        else:\n",
    "            features = np.concatenate((features, row), axis=0)\n",
    "    return features\n",
    "\n",
    "X_train = extract_features(train_eeg1_raw, train_eeg2_raw, train_emg_raw)\n",
    "\n",
    "\n",
    "if not PROTOTYPING:\n",
    "    X_test = extract_features(test_eeg1_raw, test_eeg2_raw, test_emg_raw)\n",
    "    print(\"X_test\", X_test.shape)\n",
    "print(\"X_train\", X_train.shape)\n",
    "\n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save features for future imports\n",
    "pd.DataFrame.to_csv(pd.DataFrame(X_train), 'files/eeg_feats_train.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(X_test), 'files/eeg_feats_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain features by simply doing a FFT on the data\n",
    "# probably more suitable for a neural network approach\n",
    "eeg1_freqs_train = []\n",
    "eeg2_freqs_train = []\n",
    "eeg1_freqs_test  = []\n",
    "eeg2_freqs_test  = []\n",
    "for i in range(train_eeg1_raw.shape[0]):\n",
    "    eeg1_freqs_train.append(np.real(fft(train_eeg1_raw[i])))\n",
    "    eeg2_freqs_train.append(np.real(fft(train_eeg2_raw[i])))\n",
    "    \n",
    "for i in range(test_eeg1_raw.shape[0]):\n",
    "    eeg1_freqs_test.append(np.real(fft(test_eeg1_raw[i])))\n",
    "    eeg2_freqs_test.append(np.real(fft(test_eeg2_raw[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  4.171844482421875\n"
     ]
    }
   ],
   "source": [
    "# concatenate frequency fetures from fft\n",
    "start = time.time()\n",
    "eeg_freqs_train = np.array(np.column_stack((eeg1_freqs_train, eeg2_freqs_train)))\n",
    "eeg_freqs_test  = np.array(np.column_stack((eeg1_freqs_test, eeg2_freqs_test)))\n",
    "print(\"Time: \", time.time() - start)\n",
    "\n",
    "# save features for future imports\n",
    "pd.DataFrame.to_csv(pd.DataFrame(eeg_freqs_train), 'files/eeg_freqs_train.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(eeg_freqs_test), 'files/eeg_freqs_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### homemade Feature Extraction for EMG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even more features: https://ieeexplore.ieee.org/document/7748960\n",
    "# find out if we were provided with some threshold or so (some feats need one)\n",
    "\n",
    "# functions are implemented from this paper:\n",
    "# https://www.researchgate.net/publication/323587464_A_Comprehensive_Study_on_EMG_Feature_Extraction_and_Classifiers\n",
    "# https://www.researchgate.net/publication/224148281_Evaluation_of_EMG_Feature_Extraction_for_Hand_Movement_Recognition_Based_on_Euclidean_Distance_and_Standard_Deviation\n",
    "\n",
    "# Functions for the TIME Domain\n",
    "\n",
    "# integrated EMG is the area under the rectified EMG signal \n",
    "def IEMG(signal):\n",
    "    iemg = np.sum(np.abs(signal))\n",
    "    return iemg\n",
    "\n",
    "# Mean Absolute Value\n",
    "# PRE : Requires rectified signal\n",
    "def MAV(signal, N):\n",
    "    mav = np.sum(np.abs(signal))/N\n",
    "    return mav\n",
    "\n",
    "\n",
    "# Mean Absolute Value Slope  (potentially computationally very expensive)\n",
    "def MAVS(signal, N):\n",
    "    temp = 0\n",
    "    for i in range(signal.shape[0]-1):\n",
    "        temp += np.abs(signal[i+1] - signal[i])\n",
    "    mavs = temp/N\n",
    "    return mavs\n",
    "\n",
    "\n",
    "# modified mean absolute value type 1\n",
    "def MAV1(signal, N):\n",
    "    # interval borders\n",
    "    lower = 0.25 * N\n",
    "    upper = 0.75 * N \n",
    "    temp  = 0\n",
    "    for i in range(signal.shape[0]):\n",
    "        if i >= lower and i <= upper:\n",
    "            temp += 1 * np.abs(signal[i])\n",
    "        else:\n",
    "            temp += 0.5 * np.abs(signal[i])\n",
    "    mav1 = temp/N\n",
    "    return mav1\n",
    "\n",
    "\n",
    "# modified mean absolute value type 2\n",
    "def MAV2(signal, N):\n",
    "    # interval borders\n",
    "    lower = 0.25 * N\n",
    "    upper = 0.75 * N \n",
    "    temp  = 0\n",
    "    for i in range(signal.shape[0]):\n",
    "        if i >= lower and i <= upper:\n",
    "            temp += 1 * np.abs(signal[i])\n",
    "        elif i < lower:\n",
    "            temp += (4*i/N) * np.abs(signal[i])\n",
    "        elif i > upper:\n",
    "            temp += (4*(i-N)/N) * np.abs(signal[i])\n",
    "        \n",
    "    mav2 = temp/N\n",
    "    return mav2\n",
    "\n",
    "\n",
    "# Simple Square Integral (SSI) expresses the energy of the EMG signal\n",
    "# PRE : Requires rectified signal\n",
    "def SSI(signal, N):\n",
    "    ssi = np.sum(np.abs(signal)**2)/N  # should square every value in signal element-wise\n",
    "    return ssi\n",
    "\n",
    "# The variance of EMG signal\n",
    "# PRE : Requires rectified signal\n",
    "def VAREMG(signal, N):\n",
    "    varemg = np.sum(signal**2)/(N-1)  # should square every value in signal element-wise\n",
    "    return varemg\n",
    "\n",
    "# Root Mean Square\n",
    "# PRE : Requires rectified signal\n",
    "def RMS(signal, N):\n",
    "    rms = np.sqrt(np.sum(np.abs(signal)**2)/N)  # should square every value in signal element-wise\n",
    "    return rms\n",
    "\n",
    "# the 3rd temporal moment\n",
    "def TM3(signal, N):\n",
    "    tm3 = np.sum(np.abs(signal**3))/N\n",
    "    return tm3\n",
    "\n",
    "# the 4th temporal moment\n",
    "def TM4(signal, N):\n",
    "    tm4 = np.sum(np.abs(signal**4))/N\n",
    "    return tm4\n",
    "\n",
    "# the 5th temporal moment\n",
    "def TM5(signal, N):\n",
    "    tm5 = np.sum(np.abs(signal**5))/N\n",
    "    return tm5\n",
    "\n",
    "# Waveform Length\n",
    "def WL(signal, N):\n",
    "    wl = 0\n",
    "    temp = 0\n",
    "    for j in range(signal.shape[0]-1):\n",
    "        temp = np.abs(signal[j+1] - signal[j])\n",
    "        wl += temp\n",
    "    return wl\n",
    "\n",
    "\n",
    "# TODO : find a suitable threshold (maybe 0) => visual inspection required\n",
    "def AAC(signal, N):\n",
    "    pass\n",
    "\n",
    "def DASDV(signal, N):\n",
    "    pass\n",
    "\n",
    "def ZC(signal, N):\n",
    "    pass\n",
    "\n",
    "def SSC(signal, N):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVEMENT : Wavelet-Analysis for FExtr.:\n",
    "# https://www.researchgate.net/publication/51997893_Techniques_for_Feature_Extraction_from_EMG_Signal\n",
    "# Functions for the FREQUENCY Domain\n",
    "\n",
    "# frequency median : requires the power spectrum density\n",
    "def FMD(psd):\n",
    "    fmd = 0.5 * np.sum(psd)\n",
    "    return fmd\n",
    "\n",
    "# frequency mean : requires psd, freqs and frequency median for faster computation\n",
    "def FMN(psd, freqs, fmd):\n",
    "    fmd = fmd * 2  # simply sum of all psd elements\n",
    "    fmn = np.sum(np.multiply(psd, freqs))/fmd\n",
    "    return fmn\n",
    "\n",
    "# same as FMD(), but based on amplitudes\n",
    "def MMFD(amplitudes):\n",
    "    mmfd = 0.5 * np.sum(amplitudes)\n",
    "    return mmfd\n",
    "\n",
    "# same as FMD(), but based on amplitudes\n",
    "def MMNF(signal, amplitudes, mmfd):\n",
    "    freqs = np.fft.fftfreq(amplitudes.size)  # freqs based on fourier transform\n",
    "    mmnf = np.sum(np.multiply(amplitudes, freqs))/mmfd\n",
    "    return mmnf\n",
    "    \n",
    "\n",
    "# estimate the AR coefficients of k-th order (k=6 based on literature research)\n",
    "def AR(signal, order=6):\n",
    "    ar, _, _ = arburg(signal, order)  # only save AR coefs\n",
    "    return ar\n",
    "\n",
    "# Wavelets analysis\n",
    "# import pywt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE : raw emg signal\n",
    "# POST: returns the extracted features\n",
    "def extract_features_emg(data):\n",
    "    N = data.shape[0]\n",
    "    #onsets_list = []  # save onsets of EMG signals\n",
    "    #filtered_list = []\n",
    "    # generate more features\n",
    "    mav_list = []\n",
    "    ssi_list = []\n",
    "    vemg_list= []\n",
    "    rms_list = []\n",
    "    wl_list  = []\n",
    "    iemg_list= []\n",
    "    mavs_list= []\n",
    "    mav1_list= []\n",
    "    mav2_list= []\n",
    "    tm3_list = []\n",
    "    tm4_list = []\n",
    "    tm5_list = []\n",
    "    fmd_list = []\n",
    "    fmn_list = []\n",
    "    mmfd_list= []\n",
    "    mmnf_list= []\n",
    "    ar_list  = []\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(data.shape[0]):\n",
    "        _, filt_emg, _ = emg.emg(signal=data[i].T, sampling_rate=512, show=False)  # obtain only filtered signal\n",
    "        freqs, psd = ss.welch(data[i], fs=512)  # get the PSD of the signal for the frequencies and amplitudes\n",
    "        amplitudes = np.abs(fft(data[i]))\n",
    "        #filtered_list.append(filt_emg)\n",
    "        #onsets_list.append(onsets_emg)\n",
    "        # compute features\n",
    "        mav_list.append(MAV(filt_emg, N))\n",
    "        ssi_list.append(SSI(filt_emg, N))\n",
    "        vemg_list.append(VAREMG(filt_emg, N))\n",
    "        rms_list.append(RMS(filt_emg, N))\n",
    "        wl_list.append(WL(filt_emg, N))\n",
    "        iemg_list.append(IEMG(filt_emg))\n",
    "        mavs_list.append(MAVS(filt_emg, N))\n",
    "        mav1_list.append(MAV1(filt_emg, N))\n",
    "        mav2_list.append(MAV2(filt_emg, N))\n",
    "        tm3_list.append(TM3(filt_emg, N))\n",
    "        tm4_list.append(TM4(filt_emg, N))\n",
    "        tm5_list.append(TM5(filt_emg, N))\n",
    "        fmd_res = FMD(psd)\n",
    "        fmd_list.append(fmd_res)\n",
    "        fmn_list.append(FMN(psd, freqs, fmd_res))\n",
    "        mmfd_res = MMFD(amplitudes)\n",
    "        mmfd_list.append(mmfd_res)\n",
    "        mmnf_list.append(MMNF(data[i], amplitudes, mmfd_res))\n",
    "        ar_list.append(AR(filt_emg))\n",
    "\n",
    "    print(\"Time: \", time.time() - start)\n",
    "    emg_features = [mav_list,ssi_list,vemg_list,rms_list,wl_list,iemg_list,mavs_list,mav1_list,mav2_list,\n",
    "                    tm3_list,tm4_list,tm5_list,fmd_list,fmn_list,mmfd_list,mmnf_list,ar_list]\n",
    "    \n",
    "    return emg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1361.9812316894531\n",
      "Time:  859.1871929168701\n"
     ]
    }
   ],
   "source": [
    "# get emg features for X_train and X_test\n",
    "emg_feats_train = extract_features_emg(train_emg_raw)\n",
    "emg_feats_test  = extract_features_emg(test_emg_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the coefs and save them in separate lists\n",
    "def extract_ar_coefs(features):\n",
    "    ar_feats_0 = []\n",
    "    ar_feats_1 = []\n",
    "    ar_feats_2 = []\n",
    "    ar_feats_3 = []\n",
    "    ar_feats_4 = []\n",
    "    ar_feats_5 = []\n",
    "    # 17th idx is where the AR coefs list is in\n",
    "    # we only care for the real part. the complex part is 0j anyway\n",
    "    for i in range(len(features[16])):\n",
    "        ar_feats_0.append(np.real(features[16][i][0]))\n",
    "        ar_feats_1.append(np.real(features[16][i][1]))\n",
    "        ar_feats_2.append(np.real(features[16][i][2]))\n",
    "        ar_feats_3.append(np.real(features[16][i][3]))\n",
    "        ar_feats_4.append(np.real(features[16][i][4]))\n",
    "        ar_feats_5.append(np.real(features[16][i][5]))\n",
    "    \n",
    "    return ar_feats_0, ar_feats_1, ar_feats_2, ar_feats_3, ar_feats_4, ar_feats_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.307163953781128\n"
     ]
    }
   ],
   "source": [
    "# remove the AR features list and substitute them with the individual data lists\n",
    "# else, scaling will not work properly\n",
    "start = time.time()\n",
    "ar_feats_0, ar_feats_1, ar_feats_2, ar_feats_3, ar_feats_4, ar_feats_5 = extract_ar_coefs(emg_feats_train)\n",
    "emg_feats_train_mod = np.column_stack((np.transpose(emg_feats_train[0:16]),ar_feats_0,ar_feats_1,ar_feats_2,\n",
    "                                       ar_feats_3,ar_feats_4,ar_feats_5))\n",
    "\n",
    "ar_feats_0, ar_feats_1, ar_feats_2, ar_feats_3, ar_feats_4, ar_feats_5 = extract_ar_coefs(emg_feats_test)\n",
    "emg_feats_test_mod  = np.column_stack((np.transpose(emg_feats_test[0:16]),ar_feats_0,ar_feats_1,ar_feats_2,\n",
    "                                      ar_feats_3,ar_feats_4,ar_feats_5))\n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove certain features (which are believed to be \"bad\")\n",
    "temp = emg_feats_train_mod[:,0:6]\n",
    "temp2= emg_feats_train_mod[:,16:]\n",
    "emg_feats_train_mod2 = np.array(np.column_stack((temp, temp2)))\n",
    "\n",
    "temp_ = emg_feats_test_mod[:,0:6]\n",
    "temp2_= emg_feats_test_mod[:,16:]\n",
    "emg_feats_test_mod2 = np.array(np.column_stack((temp_, temp2_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save emg features for future imports\n",
    "pd.DataFrame.to_csv(pd.DataFrame(emg_feats_train), 'files/emg_feats_train.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(emg_feats_test), 'files/emg_feats_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full train and testsets\n",
    "X_train_ = np.array(np.column_stack((eeg_train, emg_feats_train_mod2)))\n",
    "X_test_  = np.array(np.column_stack((eeg_test, emg_feats_test_mod2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64800, 67) (64800, 1)\n",
      "Time:  0.0009963512420654297\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "def split(X_train, y_train):\n",
    "    return train_test_split(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            test_size=0.1, \n",
    "            shuffle=False, \n",
    "            random_state=0)\n",
    "\n",
    "print(X_train_.shape, train_labels_raw.shape)\n",
    "if PROTOTYPING:\n",
    "    X_train, X_test, y_train, y_test = split(X_train, train_labels_raw)\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "else:\n",
    "    y_train = train_labels_raw\n",
    "    \n",
    "    \n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.20249724388122559\n"
     ]
    }
   ],
   "source": [
    "# trick(?): scale eeg feats first, then add emg feats and scale again...\n",
    "start = time.time()\n",
    "\n",
    "def scale(X_train, X_test):\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # scale\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "X_train_s, X_test_s = scale(X_train_, X_test_)\n",
    "\n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
      "[CV] C=1.0, kernel=rbf ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1.0, kernel=rbf, score=0.926, total= 1.9min\n",
      "[CV] C=1.0, kernel=sigmoid ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... C=1.0, kernel=sigmoid, score=0.753, total= 2.3min\n",
      "[CV] C=31.622776601683793, kernel=rbf ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.1min remaining:    0.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-7d3d89b86907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rbf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ovo\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Grid with best params: %s and score %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1006\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1007\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GridSearch for SVC\n",
    "start = time.time()\n",
    "\n",
    "kernels = (\"rbf\", \"sigmoid\")\n",
    "C_values = np.logspace(0, 1.5, num=2)\n",
    "param_grid = {\"kernel\" : kernels,\n",
    "              \"C\"      : C_values}\n",
    "scoring_method = \"balanced_accuracy\"\n",
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "\n",
    "classifier = SVC(kernel=\"rbf\", class_weight=\"balanced\", gamma=\"auto\", decision_function_shape=\"ovo\")\n",
    "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring=scoring_method, cv=cv, verbose=11)\n",
    "grid.fit(X_train_s, np.ravel(y_train))\n",
    "best = grid.best_params_\n",
    "print(\"Grid with best params: %s and score %f\" % (grid.best_params_, grid.best_score_))\n",
    "print(\"Time: \", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.08929613, 2.15995371, 2.23300085, 2.30851836, 2.38658979,\n",
       "       2.4673015 , 2.55074278, 2.63700596, 2.72618645, 2.81838293])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing logspace for more refined C estimates\n",
    "np.logspace(0.32,0.45,num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\made_\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  165.16580438613892\n"
     ]
    }
   ],
   "source": [
    "# SVM approach\n",
    "start = time.time()\n",
    "\n",
    "classifier = SVC(C=1, class_weight=\"balanced\", gamma=\"auto\", decision_function_shape=\"ovo\")\n",
    "classifier.fit(X_train_s, np.ravel(y_train))\n",
    "y_predict = classifier.predict(X_test_s)\n",
    "\n",
    "if PROTOTYPING:\n",
    "    print(balanced_accuracy_score(y_test, y_predict))\n",
    "    \n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test best params for classifier\n",
    "#classifier.fit(X_train_s, y_train)  # determine classifier based on best params\n",
    "#y_predict = classifier.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging\n",
    "y_train = train_labels_raw\n",
    "start = time.time()\n",
    "\n",
    "classifier = BaggingClassifier(SVC(C=1, class_weight=\"balanced\", gamma=\"auto\", decision_function_shape=\"ovo\"),\n",
    "                               n_estimators=100, random_state=0)\n",
    "classifier.fit(X_train_s, np.ravel(y_train))\n",
    "y_predict = classifier.predict(X_test_s)\n",
    "\n",
    "if PROTOTYPING:\n",
    "    print(balanced_accuracy_score(y_test, y_predict))\n",
    "    \n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale freqs from fft alone\n",
    "eeg_freqs_train_s, eeg_freqs_test_s = scale(eeg_freqs_train, eeg_freqs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "64800/64800 [==============================] - 25s 393us/step - loss: 0.4172 - acc: 0.8698\n",
      "Epoch 2/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.1730 - acc: 0.9432\n",
      "Epoch 3/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.1466 - acc: 0.9492\n",
      "Epoch 4/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.1373 - acc: 0.9523\n",
      "Epoch 5/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.1329 - acc: 0.9530\n",
      "Epoch 6/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1296 - acc: 0.9540\n",
      "Epoch 7/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.1270 - acc: 0.9545\n",
      "Epoch 8/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1249 - acc: 0.9556\n",
      "Epoch 9/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.1229 - acc: 0.9560\n",
      "Epoch 10/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1215 - acc: 0.9567\n",
      "Epoch 11/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.1206 - acc: 0.9566\n",
      "Epoch 12/100\n",
      "64800/64800 [==============================] - 2s 37us/step - loss: 0.1187 - acc: 0.9572\n",
      "Epoch 13/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.1163 - acc: 0.9582\n",
      "Epoch 14/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.1151 - acc: 0.9585\n",
      "Epoch 15/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1152 - acc: 0.9583\n",
      "Epoch 16/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1131 - acc: 0.9588\n",
      "Epoch 17/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.1135 - acc: 0.9584\n",
      "Epoch 18/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.1111 - acc: 0.9596\n",
      "Epoch 19/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.1099 - acc: 0.9602\n",
      "Epoch 20/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.1083 - acc: 0.9599\n",
      "Epoch 21/100\n",
      "64800/64800 [==============================] - 2s 36us/step - loss: 0.1066 - acc: 0.9611\n",
      "Epoch 22/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.1052 - acc: 0.9619\n",
      "Epoch 23/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.1054 - acc: 0.9616\n",
      "Epoch 24/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.1038 - acc: 0.9624\n",
      "Epoch 25/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.1023 - acc: 0.9629\n",
      "Epoch 26/100\n",
      "64800/64800 [==============================] - 2s 35us/step - loss: 0.1007 - acc: 0.9632\n",
      "Epoch 27/100\n",
      "64800/64800 [==============================] - 3s 39us/step - loss: 0.0990 - acc: 0.9638\n",
      "Epoch 28/100\n",
      "64800/64800 [==============================] - 2s 35us/step - loss: 0.0987 - acc: 0.9636\n",
      "Epoch 29/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0996 - acc: 0.9633\n",
      "Epoch 30/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0953 - acc: 0.9656\n",
      "Epoch 31/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0942 - acc: 0.9656\n",
      "Epoch 32/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0913 - acc: 0.9668\n",
      "Epoch 33/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0911 - acc: 0.9664\n",
      "Epoch 34/100\n",
      "64800/64800 [==============================] - 2s 35us/step - loss: 0.0904 - acc: 0.9668\n",
      "Epoch 35/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.0900 - acc: 0.9661\n",
      "Epoch 36/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0890 - acc: 0.9670\n",
      "Epoch 37/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0850 - acc: 0.9687\n",
      "Epoch 38/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0843 - acc: 0.9690\n",
      "Epoch 39/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0844 - acc: 0.9688\n",
      "Epoch 40/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0848 - acc: 0.9690\n",
      "Epoch 41/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0823 - acc: 0.9697\n",
      "Epoch 42/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0765 - acc: 0.9716\n",
      "Epoch 43/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0749 - acc: 0.9727\n",
      "Epoch 44/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0778 - acc: 0.9713\n",
      "Epoch 45/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0745 - acc: 0.9733\n",
      "Epoch 46/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0707 - acc: 0.9739\n",
      "Epoch 47/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0707 - acc: 0.9747\n",
      "Epoch 48/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0689 - acc: 0.9750\n",
      "Epoch 49/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0700 - acc: 0.9742\n",
      "Epoch 50/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.0703 - acc: 0.9738\n",
      "Epoch 51/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.0667 - acc: 0.9751\n",
      "Epoch 52/100\n",
      "64800/64800 [==============================] - 2s 33us/step - loss: 0.0635 - acc: 0.9769\n",
      "Epoch 53/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0688 - acc: 0.9742\n",
      "Epoch 54/100\n",
      "64800/64800 [==============================] - 3s 44us/step - loss: 0.0641 - acc: 0.9760\n",
      "Epoch 55/100\n",
      "64800/64800 [==============================] - 3s 40us/step - loss: 0.0588 - acc: 0.9785: 0s - loss: 0.0602 - acc: 0.\n",
      "Epoch 56/100\n",
      "64800/64800 [==============================] - 2s 37us/step - loss: 0.0550 - acc: 0.9807\n",
      "Epoch 57/100\n",
      "64800/64800 [==============================] - 3s 42us/step - loss: 0.0535 - acc: 0.9815\n",
      "Epoch 58/100\n",
      "64800/64800 [==============================] - 2s 38us/step - loss: 0.0507 - acc: 0.9821\n",
      "Epoch 59/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0501 - acc: 0.9821\n",
      "Epoch 60/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0514 - acc: 0.9807\n",
      "Epoch 61/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0475 - acc: 0.9830\n",
      "Epoch 62/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0443 - acc: 0.9846\n",
      "Epoch 63/100\n",
      "64800/64800 [==============================] - 3s 45us/step - loss: 0.0472 - acc: 0.9833\n",
      "Epoch 64/100\n",
      "64800/64800 [==============================] - 3s 45us/step - loss: 0.0491 - acc: 0.9822\n",
      "Epoch 65/100\n",
      "64800/64800 [==============================] - 3s 42us/step - loss: 0.0441 - acc: 0.9838\n",
      "Epoch 66/100\n",
      "64800/64800 [==============================] - 3s 43us/step - loss: 0.0449 - acc: 0.9835\n",
      "Epoch 67/100\n",
      "64800/64800 [==============================] - 3s 39us/step - loss: 0.0433 - acc: 0.9844\n",
      "Epoch 68/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0391 - acc: 0.9864\n",
      "Epoch 69/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0359 - acc: 0.9875\n",
      "Epoch 70/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0345 - acc: 0.9885\n",
      "Epoch 71/100\n",
      "64800/64800 [==============================] - 2s 37us/step - loss: 0.0393 - acc: 0.9857\n",
      "Epoch 72/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0367 - acc: 0.9872\n",
      "Epoch 73/100\n",
      "64800/64800 [==============================] - 2s 35us/step - loss: 0.0363 - acc: 0.9877\n",
      "Epoch 74/100\n",
      "64800/64800 [==============================] - 2s 35us/step - loss: 0.0352 - acc: 0.9879\n",
      "Epoch 75/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0331 - acc: 0.9885\n",
      "Epoch 76/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0293 - acc: 0.9903\n",
      "Epoch 77/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0280 - acc: 0.9907\n",
      "Epoch 78/100\n",
      "64800/64800 [==============================] - 2s 37us/step - loss: 0.0262 - acc: 0.9914\n",
      "Epoch 79/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0243 - acc: 0.9927\n",
      "Epoch 80/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0232 - acc: 0.9933\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0213 - acc: 0.9941\n",
      "Epoch 82/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0200 - acc: 0.9945\n",
      "Epoch 83/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0179 - acc: 0.9958\n",
      "Epoch 84/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0164 - acc: 0.9963\n",
      "Epoch 85/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0213 - acc: 0.9932\n",
      "Epoch 86/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0198 - acc: 0.9938\n",
      "Epoch 87/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0183 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0174 - acc: 0.9951\n",
      "Epoch 89/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0188 - acc: 0.9938\n",
      "Epoch 90/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0168 - acc: 0.9951\n",
      "Epoch 91/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0152 - acc: 0.9960\n",
      "Epoch 92/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0141 - acc: 0.9963\n",
      "Epoch 93/100\n",
      "64800/64800 [==============================] - 2s 34us/step - loss: 0.0124 - acc: 0.9972\n",
      "Epoch 94/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0105 - acc: 0.9982\n",
      "Epoch 95/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0092 - acc: 0.9983\n",
      "Epoch 96/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0108 - acc: 0.9974\n",
      "Epoch 97/100\n",
      "64800/64800 [==============================] - 2s 29us/step - loss: 0.0161 - acc: 0.9947\n",
      "Epoch 98/100\n",
      "64800/64800 [==============================] - 2s 30us/step - loss: 0.0549 - acc: 0.9802\n",
      "Epoch 99/100\n",
      "64800/64800 [==============================] - 2s 31us/step - loss: 0.0355 - acc: 0.9863\n",
      "Epoch 100/100\n",
      "64800/64800 [==============================] - 2s 32us/step - loss: 0.0211 - acc: 0.9927\n"
     ]
    }
   ],
   "source": [
    "# neural network approach\n",
    "# define the model architecture\n",
    "ann = Sequential()\n",
    "ann.add(Dense(512, input_dim = np.shape(X_train_s)[1], activation = 'relu'))\n",
    "#ann.add(Dropout(0.3))\n",
    "#ann.add(BatchNormalization())\n",
    "#ann.add(Dense(512, activation = 'relu'))\n",
    "#ann.add(Dropout(0.1))\n",
    "#ann.add(BatchNormalization())\n",
    "ann.add(Dense(256, activation = 'relu'))\n",
    "#ann.add(Dropout(0.25))\n",
    "#ann.add(BatchNormalization())\n",
    "ann.add(Dense(128, activation = 'relu'))\n",
    "#ann.add(Dropout(0.2))\n",
    "#ann.add(BatchNormalization())\n",
    "# final output layer\n",
    "ann.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr = 1e-3, decay = 1e-5)\n",
    "\n",
    "ann.compile(loss = 'sparse_categorical_crossentropy',\n",
    "            optimizer = opt,\n",
    "            metrics = ['accuracy'])\n",
    "\n",
    "# fit and predict\n",
    "ann.fit(X_train_s, y_train, epochs = 100, batch_size = 5000, class_weight = 'balanced')\n",
    "y_predict = ann.predict_classes(X_test_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.5989737510681152\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output = pd.read_csv('files/sample.csv')\n",
    "for i in range(output.shape[0]):\n",
    "    output.iat[i, 1] = y_predict[i]\n",
    "output.to_csv(\"files/SVM_OvO_3_otherfeats_bagged.csv\", index=False)\n",
    "        \n",
    "print(\"Time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64800\n"
     ]
    }
   ],
   "source": [
    "print(len(onsets_list))  # perhaps use #onsets also as a feature?\n",
    "#for item in onsets_list:\n",
    "#    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add EMG features to X_train\n",
    "# if fourier freqs are used as feats, concatenation takes really long...\n",
    "X_train_ = np.column_stack((eeg1_freqs_train,eeg2_freqs_train,mav_list,ssi_list,vemg_list,rms_list,wl_list,iemg_list,mavs_list,mav1_list,\n",
    "                            mav2_list,fmd_list,fmn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add EMG features to X_test\n",
    "X_test_ = np.column_stack((eeg1_freqs_test,eeg2_freqs_test,mav_list,ssi_list,vemg_list,rms_list,wl_list,iemg_list,mavs_list,mav1_list,\n",
    "                           mav2_list,fmd_list,fmn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43200,  1035])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.real(X_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3826554 +0.j -0.02521546+0.j -0.13306799+0.j -0.14358056+0.j\n",
      " -0.16622746+0.j]\n"
     ]
    }
   ],
   "source": [
    "xx = AR(train_emg_raw[0], order=5)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.84194114+0.j, 2.54266141+0.j, 2.69612235+0.j, 2.17536706-0.j,\n",
       "       1.34618836-0.j, 0.51530807-0.j])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emg_feats_train[11][50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64800, 11)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.transpose(emg_feats_train[0:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.154434690031882, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2.154434690031882, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64800, 10)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(emg_feats_train_mod[:,6:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43200, 12)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(emg_feats_test_mod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
